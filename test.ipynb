{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating that attention in attention requires normalizing by dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 128\n",
      "12 128\n",
      "12 128\n",
      "12 128\n"
     ]
    }
   ],
   "source": [
    "n = 12\n",
    "trials = 128\n",
    "dims = [2**i for i in range(0, n)]\n",
    "dims = np.array(dims)\n",
    "\n",
    "\n",
    "k_matrices = [[np.random.randn(d, d) for _ in range(trials)] for d in dims]\n",
    "q_matrices = [[np.random.randn(d, d) for _ in range(trials)] for d in dims]\n",
    "attn = [[k_matrices[i][j] @ q_matrices[i][j] for j in range(trials)] for i in range(n)]\n",
    "attn_norm = [[attn[i][j] / np.sqrt(dims[i]) for j in range(trials)] for i in range(n)]\n",
    "\n",
    "# print shapes of the lists\n",
    "print(len(k_matrices), len(k_matrices[0]))\n",
    "print(len(q_matrices), len(q_matrices[0]))\n",
    "print(len(attn), len(attn[0]))\n",
    "print(len(attn_norm), len(attn_norm[0]))\n",
    "\n",
    "# calculate average variance of each matrix\n",
    "k_variances = np.array([[np.var(k) for k in ks] for ks in k_matrices])\n",
    "q_variances = np.array([[np.var(q) for q in qs] for qs in q_matrices])\n",
    "attn_variances = np.array([[np.var(a) for a in attns] for attns in attn])\n",
    "attn_norm_variances = np.array([[np.var(a) for a in attns] for attns in attn_norm])\n",
    "\n",
    "# get the average variance for each dimension\n",
    "avg_k_variances = np.mean(k_variances, axis = 1)\n",
    "avg_q_variances = np.mean(q_variances, axis = 1)\n",
    "avg_attn_variances = np.mean(attn_variances, axis = 1)\n",
    "avg_attn_norm_variances = np.mean(attn_norm_variances, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.73156302 0.90093749 0.98656246 0.97328748 0.99624058\n",
      " 0.99578033 0.99856178 1.00077761 0.99994411]\n",
      "[0.         0.61325316 0.92942774 0.96390262 0.98713247 1.0047758\n",
      " 1.00273199 0.99914479 0.99964108 1.00066139]\n",
      "[  0.           1.3478741    3.651688     8.15593932  15.58760007\n",
      "  31.95728549  64.80275935 128.12791432 256.15436248 512.51734513]\n",
      "[0.         0.67393705 0.912922   1.01949242 0.974225   0.99866517\n",
      " 1.01254311 1.00099933 1.00060298 1.00101044]\n"
     ]
    }
   ],
   "source": [
    "print(avg_k_variances)\n",
    "print(avg_q_variances)\n",
    "print(avg_attn_variances)\n",
    "print(avg_attn_norm_variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
